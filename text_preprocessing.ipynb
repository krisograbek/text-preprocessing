{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, poland is a very beautiful country!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_with_upper = \"Hello, POLAND is a Very BeautiFul CountrY!\"\n",
    "text_with_upper.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Most of them use `string.punctuation`**\n",
    "\n",
    "\n",
    "\n",
    "Based on the [StackOverflow question](https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string)\n",
    "\n",
    "I ordered the solutions from the fastest to the slowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts_text = \"HI!!! I overuse ... !(@*punctuations +-*/ and*(& other signs __*(&!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using `str.translate()` \n",
    "Probably the fastest way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HI I overuse  punctuations  and other signs '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puncts_text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `re`\n",
    "\n",
    "`re` stands for Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1 with string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HI I overuse  punctuations  and other signs '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "regex.sub('', puncts_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1 in a single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HI I overuse  punctuations  and other signs '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(f'[{re.escape(string.punctuation)}]','', puncts_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2\n",
    "\n",
    "Removes **not words** and **not spaces**\n",
    "\n",
    "Note: `re` treats underscore as a word so its results are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HI I overuse  punctuations  and other signs __'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'[^\\w\\s]','', puncts_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HI I overuse  punctuations  and other signs '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a slower solution\n",
    "exclude = set(string.punctuation)\n",
    "\"\".join(ch for ch in puncts_text if ch not in exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `str.replace()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HI I overuse  punctuations  and other signs '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = puncts_text\n",
    "for c in string.punctuation:\n",
    "    clean_text = clean_text.replace(c, \"\")\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numbers Removal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_numbers = '12abcd405'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `str.translate` with `string.digits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcd'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_numbers.translate(str.maketrans('', '', string.digits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `re`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcd'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'\\d+', '', text_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcd'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'[0-9]+', '', text_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `join()` and NOT `isdigit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcd'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(i for i in text_numbers if not i.isdigit())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `join()` and `isalpha()`\n",
    "\n",
    "This actually isn't the correct solution, because `isalpha()` is True only for letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcd'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(i for i in text_numbers if i.isalpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Tags Removal\n",
    "\n",
    "Solutions from [Stack Overflow](https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_text = \"\"\"<tr class=\"color-5 negri a-bottom\">\n",
    "<td class=\"a-center\" width=\"11%\"><div style=\"min-width: 80px\">3-Pointers</div></td>\n",
    "<td><div class=\"left\" style=\"min-width: 120px; max-width:175px; width: 57%\">\n",
    "<div class=\"left margen-l2\">Player</div>\n",
    "<div class=\"right\"> Team</div>\n",
    "</div>\n",
    "</td>\n",
    "<td><div style=\"min-width: 60px; \">Season</div></td>\n",
    "<td><div class=\"\">W/L Game</div>\n",
    "</td>\n",
    "</tr>\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `re`\n",
    "\n",
    " - begin with tag opening '<'\n",
    " - then not '<'\n",
    " - not '<' at least once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n3-Pointers\\n\\nPlayer\\n Team\\n\\n\\nSeason\\nW/L Game\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('<[^<]+?>', '', html_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `BeautifulSoup`\n",
    "\n",
    " - `get_text()` removes HTML tags\n",
    " - `strip = True` removes whitespaces and newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3-Pointers,Player,Team,Season,W/L Game'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(html_text, 'html.parser')\n",
    "soup.get_text(\",\", strip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_comment = \"<img<!-- --> src=x onerror=alert(1);//><!-- -->\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both solutions fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<img src=x onerror=alert(1);//>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('<[^<]+?>', '', html_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'src=x onerror=alert(1);//>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(html_comment, 'html.parser')\n",
    "soup.get_text(\",\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' src=x onerror=alert(1);//>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_re = re.compile(r'(<!--.*?-->|<[^>]*>)')\n",
    "no_tags = tag_re.sub('', html_comment)\n",
    "no_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' src=x onerror=alert(1);//&gt;'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import html\n",
    "\n",
    "html.escape(no_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL Removal\n",
    "\n",
    "From this [StackOverflow Question](https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python/11332580)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_url = \"\"\"text1\n",
    "text2\n",
    "http://url.com/bla1/blah1/\n",
    "text3\n",
    "text4\n",
    "http://url.com/bla2/blah2/\n",
    "text5\n",
    "text6\n",
    "http://url.com/bla3/blah3/\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text1\\ntext2\\n\\ntext3\\ntext4\\n\\ntext5\\ntext6\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'http\\S+', '', text_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newlines, spaces and tabs removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `str.split()` and `join()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to Remove all white spaces, new lines and tabs'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str=\"I want to Remove all white \\t\\n\\n\\r spaces, new lines \\n and tabs \\t\"\n",
    "\" \".join(my_str.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `re`\n",
    "\n",
    " - **`\\s` stands for whitespace character, equivalent to `[ \\n\\r\\t\\f]`**\n",
    " - **`\\S` stands for not whitespace character, equivalent to `[^\\s]`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to Remove all white spaces, new lines and tabs '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('\\s+', ' ', my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to Remove all white spaces, new lines and tabs '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[^\\S]+', ' ', my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to Remove all white spaces, new lines and tabs '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[\\t\\n\\r\\f ]+', ' ', my_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `re.findall()`\n",
    "\n",
    "Taken from [StackOverflow](https://stackoverflow.com/questions/4697882/how-can-i-find-all-matches-to-a-regular-expression-in-python)\n",
    "\n",
    "NOTE: I believe this approach is slower than the one with `re.sub()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to Remove all white new lines and tabs '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match = re.findall('[\\w]+ ', my_str)\n",
    "\"\".join(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emojis Removal\n",
    "\n",
    "Found [here](https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b#gistcomment-3315605)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi ðŸ¤” How is your ðŸ™ˆ and ðŸ˜Œ. Have a nice weekend ðŸ’•ðŸ‘­ðŸ‘™ðŸ˜€ðŸŒ€'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_emojis = u\"Hi ðŸ¤” How is your ðŸ™ˆ and ðŸ˜Œ. Have a nice weekend ðŸ’•ðŸ‘­ðŸ‘™\\U0001F600\\U0001F300\"\n",
    "text_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi  How is your  and . Have a nice weekend '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emojis_pattern = re.compile(pattern=\"[\"\n",
    "                    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                    u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                    u\"\\U00002702-\\U000027B0\"\n",
    "                    u\"\\U00002702-\\U000027B0\"\n",
    "                    u\"\\U000024C2-\\U0001F251\"\n",
    "                    u\"\\U0001f926-\\U0001f937\"\n",
    "                    u\"\\U00010000-\\U0010ffff\"\n",
    "                    u\"\\u2640-\\u2642\"\n",
    "                    u\"\\u2600-\\u2B55\"\n",
    "                    u\"\\u200d\"\n",
    "                    u\"\\u23cf\"\n",
    "                    u\"\\u23e9\"\n",
    "                    u\"\\u231a\"\n",
    "                    u\"\\ufe0f\"  # dingbats\n",
    "                    u\"\\u3030\"\n",
    "                \"]+\", flags = re.UNICODE)\n",
    "\n",
    "emojis_pattern.sub(r'', text_emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing Accented Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Malaga, aeeohello. Polish: nNcCsSeaozZzZ letters. German uoaoss letters'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unidecode\n",
    "text_accented = \"MÃ¡laga, Ã Ã©ÃªÃ¶hello. Polish: Å„ÅƒÄ‡Ä†Å›ÅšÄ™Ä…Ã³Å¼Å»ÅºÅ¹ letters. German Ã¼Ã¶Ã¤Ã¶ÃŸ letters\"\n",
    "\n",
    "unidecode.unidecode(text_accented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling corrections\n",
    "\n",
    "Solutions from [StackOverflow](https://stackoverflow.com/questions/13928155/spell-checker-for-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `spellchecker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I really need some corrections This sentence has misspelled words'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "def correct_spelling(text):\n",
    "    corrected_text = list()\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        next_word = word\n",
    "        if word in misspelled_words:\n",
    "            next_word = spell.correction(word)\n",
    "        corrected_text.append(next_word)\n",
    "    \n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "text_misspelled = \"I realli needt smoe corection. This sentnce has mispelled wirds\"\n",
    "correct_spelling(text_misspelled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `autocorrect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really need some correction. This sentence has misspelled words\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "speller = Speller(lang='en')\n",
    "\n",
    "print(speller(text_misspelled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `textblob`\n",
    "\n",
    "Found [here](https://www.geeksforgeeks.org/python-textblob-correct-method/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I really need some correction. His sentence has dispelled words\")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "TextBlob(text_misspelled).correct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.  His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked.  \"What\\'s happened to me?\" he thought.  It wasn\\'t a dream.  His room, a proper human room although a little too small, lay peacefully between its four familiar walls.  A collection of textile samples lay spread out on the table - Samsa was a travelling salesman - and above it there hung a picture that he had recently cut out of an illustrated magazine and housed in a nice, gilded frame.  It showed a lady fitted out with a fur hat and fur boa who sat upright, raising a heavy fur muff that covered the whole of her lower arm towards the viewer.  Gregor then turned to look out the window at the dull weather.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('metamorphosis.txt', 'r') as f:\n",
    "    # without newlines\n",
    "    lines = [line.rstrip() for line in f]\n",
    "    # read text line by line including newlines \\n\n",
    "    # text_lines = f.readlines()\n",
    "\n",
    "N_LINES = 20\n",
    "text_lines = \" \".join([line for line in lines[:N_LINES]])\n",
    "text_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the whole text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found\\nhimself transformed in his bed into a horrible vermin.  He lay on\\nhis armour-like back, and if he lifted his head a little he could\\nsee his brown belly, slightly domed and divided by arches into stiff\\nsections.  The bedding was hardl'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the whole file\n",
    "with open('metamorphosis.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "N_CHARS = 300\n",
    "text_chars = text[:N_CHARS]\n",
    "text_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `re`\n",
    "\n",
    "Return the list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room', 'although', 'a', 'little', 'too', 'small', 'lay', 'peacefully', 'between', 'its', 'four', 'familiar', 'walls', 'A', 'collection', 'of', 'textile', 'samples', 'lay', 'spread', 'out', 'on', 'the', 'table', 'Samsa', 'was', 'a', 'travelling', 'salesman', 'and', 'above', 'it', 'there', 'hung', 'a', 'picture', 'that', 'he', 'had', 'recently', 'cut', 'out', 'of', 'an', 'illustrated', 'magazine', 'and', 'housed', 'in', 'a', 'nice', 'gilded', 'frame', 'It', 'showed', 'a', 'lady', 'fitted', 'out', 'with', 'a', 'fur', 'hat', 'and', 'fur', 'boa', 'who', 'sat', 'upright', 'raising', 'a', 'heavy', 'fur', 'muff', 'that', 'covered', 'the', 'whole', 'of', 'her', 'lower', 'arm', 'towards', 'the', 'viewer', 'Gregor', 'then', 'turned', 'to', 'look', 'out', 'the', 'window', 'at', 'the', 'dull', 'weather']\n"
     ]
    }
   ],
   "source": [
    "re_tokens = re.findall('[\\w]+', text_lines)\n",
    "print(len(re_tokens))\n",
    "print(re_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224 ['One', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', '.', 'He', 'lay', 'on', 'his', 'armour-like', 'back', ',', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', ',', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', '.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', '.', 'His', 'many', 'legs', ',', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', '.', '``', 'What', \"'s\", 'happened', 'to', 'me', '?', \"''\", 'he', 'thought', '.', 'It', 'was', \"n't\", 'a', 'dream', '.', 'His', 'room', ',', 'a', 'proper', 'human', 'room', 'although', 'a', 'little', 'too', 'small', ',', 'lay', 'peacefully', 'between', 'its', 'four', 'familiar', 'walls', '.', 'A', 'collection', 'of', 'textile', 'samples', 'lay', 'spread', 'out', 'on', 'the', 'table', '-', 'Samsa', 'was', 'a', 'travelling', 'salesman', '-', 'and', 'above', 'it', 'there', 'hung', 'a', 'picture', 'that', 'he', 'had', 'recently', 'cut', 'out', 'of', 'an', 'illustrated', 'magazine', 'and', 'housed', 'in', 'a', 'nice', ',', 'gilded', 'frame', '.', 'It', 'showed', 'a', 'lady', 'fitted', 'out', 'with', 'a', 'fur', 'hat', 'and', 'fur', 'boa', 'who', 'sat', 'upright', ',', 'raising', 'a', 'heavy', 'fur', 'muff', 'that', 'covered', 'the', 'whole', 'of', 'her', 'lower', 'arm', 'towards', 'the', 'viewer', '.', 'Gregor', 'then', 'turned', 'to', 'look', 'out', 'the', 'window', 'at', 'the', 'dull', 'weather', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk_tokens = word_tokenize(text_lines)\n",
    "print(len(nltk_tokens), nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `spaCy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n",
      "[One, morning, ,, when, Gregor, Samsa, woke, from, troubled, dreams, ,, he, found, himself, transformed, in, his, bed, into, a, horrible, vermin, .,  , He, lay, on, his, armour, -, like, back, ,, and, if, he, lifted, his, head, a, little, he, could, see, his, brown, belly, ,, slightly, domed, and, divided, by, arches, into, stiff, sections, .,  , The, bedding, was, hardly, able, to, cover, it, and, seemed, ready, to, slide, off, any, moment, .,  , His, many, legs, ,, pitifully, thin, compared, with, the, size, of, the, rest, of, him, ,, waved, about, helplessly, as, he, looked, .,  , \", What, 's, happened, to, me, ?, \", he, thought, .,  , It, was, n't, a, dream, .,  , His, room, ,, a, proper, human, room, although, a, little, too, small, ,, lay, peacefully, between, its, four, familiar, walls, .,  , A, collection, of, textile, samples, lay, spread, out, on, the, table, -, Samsa, was, a, travelling, salesman, -, and, above, it, there, hung, a, picture, that, he, had, recently, cut, out, of, an, illustrated, magazine, and, housed, in, a, nice, ,, gilded, frame, .,  , It, showed, a, lady, fitted, out, with, a, fur, hat, and, fur, boa, who, sat, upright, ,, raising, a, heavy, fur, muff, that, covered, the, whole, of, her, lower, arm, towards, the, viewer, .,  , Gregor, then, turned, to, look, out, the, window, at, the, dull, weather, .]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text_lines)\n",
    "spacy_tokens = list([token for token in doc])\n",
    "print(len(spacy_tokens))\n",
    "print(spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room', 'although', 'a', 'little', 'too', 'small', 'lay', 'peacefully', 'between', 'its', 'four', 'familiar', 'walls', 'A', 'collection', 'of', 'textile', 'samples', 'lay', 'spread', 'out', 'on', 'the', 'table', 'Samsa', 'was', 'a', 'travelling', 'salesman', 'and', 'above', 'it', 'there', 'hung', 'a', 'picture', 'that', 'he', 'had', 'recently', 'cut', 'out', 'of', 'an', 'illustrated', 'magazine', 'and', 'housed', 'in', 'a', 'nice', 'gilded', 'frame', 'It', 'showed', 'a', 'lady', 'fitted', 'out', 'with', 'a', 'fur', 'hat', 'and', 'fur', 'boa', 'who', 'sat', 'upright', 'raising', 'a', 'heavy', 'fur', 'muff', 'that', 'covered', 'the', 'whole', 'of', 'her', 'lower', 'arm', 'towards', 'the', 'viewer', 'Gregor', 'then', 'turned', 'to', 'look', 'out', 'the', 'window', 'at', 'the', 'dull', 'weather']\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "\n",
    "gensim_tokens = list(tokenize(text_lines))\n",
    "print(len(gensim_tokens))\n",
    "print(gensim_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Comparision\n",
    "\n",
    " - it seems that `gensim` uses the same `re` function, that we showed above. Both returned only words\n",
    " - `nltk` and `spacy` return also punctuations\n",
    " - `spacy` treats a whitespace as a token if there is a double whitespace. In our text each sentence-ending dot is followed by double whitespace. We could clean this but at least we see that `spacy` behaves differently\n",
    " - **not or n't** contraction gives different results. `spacy` and `nltk` splits *wasn't* to *was* and *n't*, whereas `re` and `gensim` to *wasn* and *t*.\n",
    " - when there is a hyphen between words, we get 3 different results. Our example is armour-like. `nltk` returns **a single token**: *armour-like*, `re` and `gensim` return **two tokens**: *armour* and *like*. `spacy` returns **three tokens**: *armour*, *-*, and *like*\n",
    " - `nltk` converts quotation marks. Quote opening: **``**, quote closing **' '**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `re`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,\n",
       " ['One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin',\n",
       "  '  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections',\n",
       "  '  The bedding was hardly able to cover it and seemed ready to slide off any moment',\n",
       "  '  His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked',\n",
       "  '  \"What\\'s happened to me',\n",
       "  '\" he thought',\n",
       "  \"  It wasn't a dream\",\n",
       "  '  His room, a proper human room although a little too small, lay peacefully between its four familiar walls',\n",
       "  '  A collection of textile samples lay spread out on the table - Samsa was a travelling salesman - and above it there hung a picture that he had recently cut out of an illustrated magazine and housed in a nice, gilded frame',\n",
       "  '  It showed a lady fitted out with a fur hat and fur boa who sat upright, raising a heavy fur muff that covered the whole of her lower arm towards the viewer',\n",
       "  '  Gregor then turned to look out the window at the dull weather',\n",
       "  ''])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_sentences = re.compile('[.?!]').split(text_lines)\n",
    "len(re_sentences), re_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 ['One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.', 'He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.', 'The bedding was hardly able to cover it and seemed ready to slide off any moment.', 'His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked.', '\"What\\'s happened to me?\"', 'he thought.', \"It wasn't a dream.\", 'His room, a proper human room although a little too small, lay peacefully between its four familiar walls.', 'A collection of textile samples lay spread out on the table - Samsa was a travelling salesman - and above it there hung a picture that he had recently cut out of an illustrated magazine and housed in a nice, gilded frame.', 'It showed a lady fitted out with a fur hat and fur boa who sat upright, raising a heavy fur muff that covered the whole of her lower arm towards the viewer.', 'Gregor then turned to look out the window at the dull weather.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk_sentences = sent_tokenize(text_lines)\n",
    "print(len(nltk_sentences), nltk_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `spaCy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,\n",
       " [One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.,\n",
       "   He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.,\n",
       "   The bedding was hardly able to cover it and seemed ready to slide off any moment.,\n",
       "   His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked.,\n",
       "   \"What's happened to me?\",\n",
       "  he thought.,\n",
       "   It wasn't a dream.,\n",
       "   His room, a proper human room although a little too small, lay peacefully between its four familiar walls.,\n",
       "   A collection of textile samples lay spread out on the table - Samsa was a travelling salesman - and above it there hung a picture that he had recently cut out of an illustrated magazine and housed in a nice, gilded frame.,\n",
       "   It showed a lady fitted out with a fur hat and fur boa who sat upright, raising a heavy fur muff that covered the whole of her lower arm towards the viewer.,\n",
       "   Gregor then turned to look out the window at the dull weather.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text_lines)\n",
    "spacy_sentences = list([sent for sent in doc.sents])\n",
    "len(spacy_sentences), spacy_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll ignore punctuations. Tokenization step using `re` gives us exactly that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 ['One', 'morning', 'Gregor', 'Samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armour', 'like', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'seemed', 'ready', 'slide', 'moment', 'many', 'legs', 'pitifully', 'thin', 'compared', 'size', 'rest', 'waved', 'helplessly', 'looked', 'happened', 'thought', 'dream', 'room', 'proper', 'human', 'room', 'although', 'little', 'small', 'lay', 'peacefully', 'four', 'familiar', 'walls', 'collection', 'textile', 'samples', 'lay', 'spread', 'table', 'Samsa', 'travelling', 'salesman', 'hung', 'picture', 'recently', 'cut', 'illustrated', 'magazine', 'housed', 'nice', 'gilded', 'frame', 'showed', 'lady', 'fitted', 'fur', 'hat', 'fur', 'boa', 'sat', 'upright', 'raising', 'heavy', 'fur', 'muff', 'covered', 'whole', 'lower', 'arm', 'towards', 'viewer', 'Gregor', 'turned', 'look', 'window', 'dull', 'weather']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_nltk = stopwords.words('english')\n",
    "# print(len(stop_words_nltk),stop_words_nltk)\n",
    "\n",
    "filtered_nltk = [word for word in re_tokens if word.lower() not in stop_words_nltk]\n",
    "print(len(filtered_nltk), filtered_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `spaCy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 ['morning', 'Gregor', 'Samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armour', 'like', 'lifted', 'head', 'little', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'ready', 'slide', 'moment', 'legs', 'pitifully', 'thin', 'compared', 'size', 'rest', 'waved', 'helplessly', 'looked', 's', 'happened', 'thought', 'wasn', 't', 'dream', 'room', 'proper', 'human', 'room', 'little', 'small', 'lay', 'peacefully', 'familiar', 'walls', 'collection', 'textile', 'samples', 'lay', 'spread', 'table', 'Samsa', 'travelling', 'salesman', 'hung', 'picture', 'recently', 'cut', 'illustrated', 'magazine', 'housed', 'nice', 'gilded', 'frame', 'showed', 'lady', 'fitted', 'fur', 'hat', 'fur', 'boa', 'sat', 'upright', 'raising', 'heavy', 'fur', 'muff', 'covered', 'lower', 'arm', 'viewer', 'Gregor', 'turned', 'look', 'window', 'dull', 'weather']\n"
     ]
    }
   ],
   "source": [
    "stop_words_spacy = nlp.Defaults.stop_words\n",
    "\n",
    "filtered_spacy = [word for word in re_tokens if word.lower() not in stop_words_spacy]\n",
    "print(len(filtered_spacy), filtered_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 ['morning', 'Gregor', 'Samsa', 'woke', 'troubled', 'dreams', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armour', 'like', 'lifted', 'head', 'little', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'ready', 'slide', 'moment', 'legs', 'pitifully', 'compared', 'size', 'rest', 'waved', 'helplessly', 'looked', 's', 'happened', 'thought', 'wasn', 't', 'dream', 'room', 'proper', 'human', 'room', 'little', 'small', 'lay', 'peacefully', 'familiar', 'walls', 'collection', 'textile', 'samples', 'lay', 'spread', 'table', 'Samsa', 'travelling', 'salesman', 'hung', 'picture', 'recently', 'cut', 'illustrated', 'magazine', 'housed', 'nice', 'gilded', 'frame', 'showed', 'lady', 'fitted', 'fur', 'hat', 'fur', 'boa', 'sat', 'upright', 'raising', 'heavy', 'fur', 'muff', 'covered', 'lower', 'arm', 'viewer', 'Gregor', 'turned', 'look', 'window', 'dull', 'weather']\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "stop_words_gensim = STOPWORDS\n",
    "\n",
    "filtered_gensim = [word for word in re_tokens if word.lower() not in stop_words_gensim]\n",
    "print(len(filtered_gensim), filtered_gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method from gensim using the `remove_stopwords` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713 One morning, Gregor Samsa woke troubled dreams, transformed bed horrible vermin. He lay armour-like back, lifted head little brown belly, slightly domed divided arches stiff sections. The bedding hardly able cover ready slide moment. His legs, pitifully compared size rest him, waved helplessly looked. \"What's happened me?\" thought. It wasn't dream. His room, proper human room little small, lay peacefully familiar walls. A collection textile samples lay spread table - Samsa travelling salesman - hung picture recently cut illustrated magazine housed nice, gilded frame. It showed lady fitted fur hat fur boa sat upright, raising heavy fur muff covered lower arm viewer. Gregor turned look window dull weather.\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "filtered_sentence_gensim = remove_stopwords(text_lines)\n",
    "print(len(filtered_sentence_gensim), filtered_sentence_gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`nltk` vs `spacy`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seemed',\n",
       " 'One',\n",
       " 'wasn',\n",
       " 'towards',\n",
       " 'see',\n",
       " 's',\n",
       " 'four',\n",
       " 'back',\n",
       " 'although',\n",
       " 'many',\n",
       " 'could',\n",
       " 'whole',\n",
       " 't']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(filtered_nltk) ^ set(filtered_spacy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`nltk` vs `gensim`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seemed',\n",
       " 'One',\n",
       " 'found',\n",
       " 'wasn',\n",
       " 'thin',\n",
       " 'towards',\n",
       " 'see',\n",
       " 's',\n",
       " 'four',\n",
       " 'back',\n",
       " 'although',\n",
       " 'many',\n",
       " 'could',\n",
       " 'whole',\n",
       " 't']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(filtered_nltk) ^ set(filtered_gensim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`gensim` vs `spacy`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['found', 'thin']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(filtered_gensim) ^ set(filtered_spacy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing stopwords lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"NLTK stopwords\",stop_words_nltk)\n",
    "# print(\"Spacy stopwords\",stop_words_spacy)\n",
    "# print(\"Gensim stopwords\",stop_words_gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK stopwords len 179\n",
      "Spacy stopwords len 326\n",
      "Gensim stopwords len 337\n"
     ]
    }
   ],
   "source": [
    "print(\"NLTK stopwords len\", len(stop_words_nltk))\n",
    "print(\"Spacy stopwords len\", len(stop_words_spacy))\n",
    "print(\"Gensim stopwords len\", len(stop_words_gensim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "For lemmatization we'll ignore punctuations. Tokenization step using `re` gives us exactly that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dream', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arch', 'into', 'stiff', 'section', 'The', 'bedding', 'wa', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'leg', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'a', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room', 'although', 'a', 'little', 'too', 'small', 'lay', 'peacefully', 'between', 'it', 'four', 'familiar', 'wall', 'A', 'collection', 'of', 'textile', 'sample', 'lay', 'spread', 'out', 'on', 'the', 'table', 'Samsa', 'wa', 'a', 'travelling', 'salesman', 'and', 'above', 'it', 'there', 'hung', 'a', 'picture', 'that', 'he', 'had', 'recently', 'cut', 'out', 'of', 'an', 'illustrated', 'magazine', 'and', 'housed', 'in', 'a', 'nice', 'gilded', 'frame', 'It', 'showed', 'a', 'lady', 'fitted', 'out', 'with', 'a', 'fur', 'hat', 'and', 'fur', 'boa', 'who', 'sat', 'upright', 'raising', 'a', 'heavy', 'fur', 'muff', 'that', 'covered', 'the', 'whole', 'of', 'her', 'lower', 'arm', 'towards', 'the', 'viewer', 'Gregor', 'then', 'turned', 'to', 'look', 'out', 'the', 'window', 'at', 'the', 'dull', 'weather']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "nltk_lemmas = [nltk_lemmatizer.lemmatize(w) for w in re_tokens]\n",
    "print(nltk_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `spaCy`\n",
    "\n",
    "[spacy documentation](https://spacy.io/api/lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 ['one', 'morning', 'when', 'Gregor', 'Samsa', 'wake', 'from', 'troubled', 'dream', 'he', 'find', 'himself', 'transform', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'he', 'lie', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lift', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'dome', 'and', 'divide', 'by', 'arch', 'into', 'stiff', 'section', 'the', 'bedding', 'be', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seem', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'his', 'many', 'leg', 'pitifully', 'thin', 'compare', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'he', 'wave', 'about', 'helplessly', 'as', 'he', 'look', 'what', 's', 'happen', 'to', 'I', 'he', 'think', 'it', 'wasn', 't', 'a', 'dream', 'his', 'room', 'a', 'proper', 'human', 'room', 'although', 'a', 'little', 'too', 'small', 'lay', 'peacefully', 'between', 'its', 'four', 'familiar', 'wall', 'a', 'collection', 'of', 'textile', 'sample', 'lie', 'spread', 'out', 'on', 'the', 'table', 'Samsa', 'be', 'a', 'travel', 'salesman', 'and', 'above', 'it', 'there', 'hang', 'a', 'picture', 'that', 'he', 'have', 'recently', 'cut', 'out', 'of', 'an', 'illustrate', 'magazine', 'and', 'house', 'in', 'a', 'nice', 'gild', 'frame', 'it', 'show', 'a', 'lady', 'fit', 'out', 'with', 'a', 'fur', 'hat', 'and', 'fur', 'boa', 'who', 'sit', 'upright', 'raise', 'a', 'heavy', 'fur', 'muff', 'that', 'cover', 'the', 'whole', 'of', 'her', 'low', 'arm', 'towards', 'the', 'viewer', 'Gregor', 'then', 'turn', 'to', 'look', 'out', 'the', 'window', 'at', 'the', 'dull', 'weather']\n"
     ]
    }
   ],
   "source": [
    "# spacy_text = \" \".join([token.text for token in spacy_tokens])\n",
    "text_from_re_tokens = \" \".join([word for word in re_tokens])\n",
    "# spacy_lemmas = [word.lemma_ for word in nlp(text_lines)]\n",
    "spacy_lemmas = [word.lemma_ for word in nlp(text_from_re_tokens)]\n",
    "print(len(spacy_lemmas), spacy_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `TextBlob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 ['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dream', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arch', 'into', 'stiff', 'section', 'The', 'bedding', 'wa', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'leg', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'a', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room', 'although', 'a', 'little', 'too', 'small', 'lay', 'peacefully', 'between', 'it', 'four', 'familiar', 'wall', 'A', 'collection', 'of', 'textile', 'sample', 'lay', 'spread', 'out', 'on', 'the', 'table', 'Samsa', 'wa', 'a', 'travelling', 'salesman', 'and', 'above', 'it', 'there', 'hung', 'a', 'picture', 'that', 'he', 'had', 'recently', 'cut', 'out', 'of', 'an', 'illustrated', 'magazine', 'and', 'housed', 'in', 'a', 'nice', 'gilded', 'frame', 'It', 'showed', 'a', 'lady', 'fitted', 'out', 'with', 'a', 'fur', 'hat', 'and', 'fur', 'boa', 'who', 'sat', 'upright', 'raising', 'a', 'heavy', 'fur', 'muff', 'that', 'covered', 'the', 'whole', 'of', 'her', 'lower', 'arm', 'towards', 'the', 'viewer', 'Gregor', 'then', 'turned', 'to', 'look', 'out', 'the', 'window', 'at', 'the', 'dull', 'weather']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob, Word\n",
    "\n",
    "# create a TextBlob for our sentence\n",
    "sent_tb = TextBlob(text_from_re_tokens)\n",
    "\n",
    "# lemmatize each word\n",
    "blob_lemmas = [word.lemmatize() for word in sent_tb.words]\n",
    "print(len(blob_lemmas), blob_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding POS Tags to `nltk` and `TextBlob`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `nltk` and `TextBlob` treat every word as a noun. This is why words like \"woke\", \"found\", or \"transformed\" don't change after the lemmatization step. We can provide more information by adding the corresponding Part of Speech for each token. \n",
    "\n",
    "In `nltk`:\n",
    "\n",
    " - we use `pos_tag()` to get tokens along with tags\n",
    " - we call the `lemmatize()` function with the second parameter, that is a tag\n",
    "\n",
    "[Source](https://www.guru99.com/stemming-lemmatization-python-nltk.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'wake', 'from', 'troubled', 'dream', 'he', 'find', 'himself', 'transform', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lift', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divide', 'by', 'arch', 'into', 'stiff', 'section', 'The', 'bedding', 'be', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seem', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'leg', 'pitifully', 'thin', 'compare', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'wave', 'about', 'helplessly', 'a', 'he', 'look', 'What', 's', 'happen', 'to', 'me', 'he', 'think', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room', 'although', 'a', 'little', 'too', 'small', 'lay', 'peacefully', 'between', 'it', 'four', 'familiar', 'wall', 'A', 'collection', 'of', 'textile', 'sample', 'lay', 'spread', 'out', 'on', 'the', 'table', 'Samsa', 'be', 'a', 'travelling', 'salesman', 'and', 'above', 'it', 'there', 'hang', 'a', 'picture', 'that', 'he', 'have', 'recently', 'cut', 'out', 'of', 'an', 'illustrated', 'magazine', 'and', 'house', 'in', 'a', 'nice', 'gild', 'frame', 'It', 'show', 'a', 'lady', 'fit', 'out', 'with', 'a', 'fur', 'hat', 'and', 'fur', 'boa', 'who', 'sit', 'upright', 'raise', 'a', 'heavy', 'fur', 'muff', 'that', 'cover', 'the', 'whole', 'of', 'her', 'low', 'arm', 'towards', 'the', 'viewer', 'Gregor', 'then', 'turn', 'to', 'look', 'out', 'the', 'window', 'at', 'the', 'dull', 'weather']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from collections import defaultdict\n",
    "\n",
    "tag_map_nltk = defaultdict(lambda : wn.NOUN)\n",
    "tag_map_nltk['J'] = wn.ADJ\n",
    "tag_map_nltk['V'] = wn.VERB\n",
    "tag_map_nltk['R'] = wn.ADV\n",
    "\n",
    "nltk_lemmas2 = [nltk_lemmatizer.lemmatize(token, tag_map_nltk[tag[0]]) for token, tag in pos_tag(re_tokens)]\n",
    "print(nltk_lemmas2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `TextBlob`:\n",
    "\n",
    " - we call `TextBlob(text).tags` to get tokens and tags\n",
    " - we call `word.lemmatize()` with the tag parameter\n",
    "\n",
    "[Source](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/#textbloblemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 ['One', 'morning', 'when', 'Gregor', 'Samsa', 'wake', 'from', 'troubled', 'dream', 'he', 'find', 'himself', 'transform', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lift', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divide', 'by', 'arch', 'into', 'stiff', 'section', 'The', 'bedding', 'be', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seem', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'leg', 'pitifully', 'thin', 'compare', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'wave', 'about', 'helplessly', 'a', 'he', 'look', 'What', 's', 'happen', 'to', 'me', 'he', 'think', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room', 'although', 'a', 'little', 'too', 'small', 'lay', 'peacefully', 'between', 'it', 'four', 'familiar', 'wall', 'A', 'collection', 'of', 'textile', 'sample', 'lay', 'spread', 'out', 'on', 'the', 'table', 'Samsa', 'be', 'a', 'travelling', 'salesman', 'and', 'above', 'it', 'there', 'hang', 'a', 'picture', 'that', 'he', 'have', 'recently', 'cut', 'out', 'of', 'an', 'illustrated', 'magazine', 'and', 'house', 'in', 'a', 'nice', 'gild', 'frame', 'It', 'show', 'a', 'lady', 'fit', 'out', 'with', 'a', 'fur', 'hat', 'and', 'fur', 'boa', 'who', 'sit', 'upright', 'raise', 'a', 'heavy', 'fur', 'muff', 'that', 'cover', 'the', 'whole', 'of', 'her', 'low', 'arm', 'towards', 'the', 'viewer', 'Gregor', 'then', 'turn', 'to', 'look', 'out', 'the', 'window', 'at', 'the', 'dull', 'weather']\n"
     ]
    }
   ],
   "source": [
    "tag_map_tb = {  \"J\": 'a', # adjectives\n",
    "                \"N\": 'n', # nouns\n",
    "                \"V\": 'v', # verbs\n",
    "                \"R\": 'r'} # adverbs\n",
    "\n",
    "words_and_tags = [(w, tag_map_tb.get(pos[0], 'n')) for w, pos in sent_tb.tags]\n",
    "blob_lemmas2 = [word.lemmatize(tag) for word, tag in words_and_tags]\n",
    "print(len(blob_lemmas2), blob_lemmas2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(nltk_lemmas) ^ set(blob_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(nltk_lemmas2) ^ set(blob_lemmas2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk` and `TextBlob` return identical results.\n",
    "\n",
    "Let's see what's changed after applying POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be',\n",
       " 'compare',\n",
       " 'compared',\n",
       " 'covered',\n",
       " 'divide',\n",
       " 'divided',\n",
       " 'find',\n",
       " 'fit',\n",
       " 'fitted',\n",
       " 'found',\n",
       " 'gild',\n",
       " 'gilded',\n",
       " 'had',\n",
       " 'hang',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'have',\n",
       " 'house',\n",
       " 'housed',\n",
       " 'hung',\n",
       " 'lift',\n",
       " 'lifted',\n",
       " 'looked',\n",
       " 'low',\n",
       " 'lower',\n",
       " 'raise',\n",
       " 'raising',\n",
       " 'sat',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'show',\n",
       " 'showed',\n",
       " 'sit',\n",
       " 'think',\n",
       " 'thought',\n",
       " 'transform',\n",
       " 'transformed',\n",
       " 'turn',\n",
       " 'turned',\n",
       " 'wa',\n",
       " 'wake',\n",
       " 'wave',\n",
       " 'waved',\n",
       " 'woke']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(set(nltk_lemmas) ^ set(nltk_lemmas2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk` did a great job at turning past tense verbs to the present tense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming with `nltk`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `PorterStemmer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morn', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubl', 'dream', 'he', 'found', 'himself', 'transform', 'in', 'hi', 'bed', 'into', 'a', 'horribl', 'vermin', 'he', 'lay', 'on', 'hi', 'armour', 'like', 'back', 'and', 'if', 'he', 'lift', 'hi', 'head', 'a', 'littl', 'he', 'could', 'see', 'hi', 'brown', 'belli', 'slightli', 'dome', 'and', 'divid', 'by', 'arch', 'into', 'stiff', 'section', 'the', 'bed', 'wa', 'hardli', 'abl', 'to', 'cover', 'it', 'and', 'seem', 'readi', 'to', 'slide', 'off', 'ani', 'moment', 'hi', 'mani', 'leg', 'piti', 'thin', 'compar', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'wave', 'about', 'helplessli', 'as', 'he', 'look', 'what', 's', 'happen', 'to', 'me', 'he', 'thought', 'it', 'wasn', 't', 'a', 'dream', 'hi', 'room', 'a', 'proper', 'human', 'room', 'although', 'a', 'littl', 'too', 'small', 'lay', 'peac', 'between', 'it', 'four', 'familiar', 'wall', 'a', 'collect', 'of', 'textil', 'sampl', 'lay', 'spread', 'out', 'on', 'the', 'tabl', 'samsa', 'wa', 'a', 'travel', 'salesman', 'and', 'abov', 'it', 'there', 'hung', 'a', 'pictur', 'that', 'he', 'had', 'recent', 'cut', 'out', 'of', 'an', 'illustr', 'magazin', 'and', 'hous', 'in', 'a', 'nice', 'gild', 'frame', 'it', 'show', 'a', 'ladi', 'fit', 'out', 'with', 'a', 'fur', 'hat', 'and', 'fur', 'boa', 'who', 'sat', 'upright', 'rais', 'a', 'heavi', 'fur', 'muff', 'that', 'cover', 'the', 'whole', 'of', 'her', 'lower', 'arm', 'toward', 'the', 'viewer', 'gregor', 'then', 'turn', 'to', 'look', 'out', 'the', 'window', 'at', 'the', 'dull', 'weather']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "ps_stemms = [ps.stem(w) for w in re_tokens]\n",
    "print(ps_stemms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morn', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubl', 'dream', 'he', 'found', 'himself', 'transform', 'in', 'his', 'bed', 'into', 'a', 'horribl', 'vermin', 'he', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lift', 'his', 'head', 'a', 'littl', 'he', 'could', 'see', 'his', 'brown', 'belli', 'slight', 'dome', 'and', 'divid', 'by', 'arch', 'into', 'stiff', 'section', 'the', 'bed', 'was', 'hard', 'abl', 'to', 'cover', 'it', 'and', 'seem', 'readi', 'to', 'slide', 'off', 'ani', 'moment', 'his', 'mani', 'leg', 'piti', 'thin', 'compar', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'wave', 'about', 'helpless', 'as', 'he', 'look', 'what', 's', 'happen', 'to', 'me', 'he', 'thought', 'it', 'wasn', 't', 'a', 'dream', 'his', 'room', 'a', 'proper', 'human', 'room', 'although', 'a', 'littl', 'too', 'small', 'lay', 'peac', 'between', 'it', 'four', 'familiar', 'wall', 'a', 'collect', 'of', 'textil', 'sampl', 'lay', 'spread', 'out', 'on', 'the', 'tabl', 'samsa', 'was', 'a', 'travel', 'salesman', 'and', 'abov', 'it', 'there', 'hung', 'a', 'pictur', 'that', 'he', 'had', 'recent', 'cut', 'out', 'of', 'an', 'illustr', 'magazin', 'and', 'hous', 'in', 'a', 'nice', 'gild', 'frame', 'it', 'show', 'a', 'ladi', 'fit', 'out', 'with', 'a', 'fur', 'hat', 'and', 'fur', 'boa', 'who', 'sat', 'upright', 'rais', 'a', 'heavi', 'fur', 'muff', 'that', 'cover', 'the', 'whole', 'of', 'her', 'lower', 'arm', 'toward', 'the', 'viewer', 'gregor', 'then', 'turn', 'to', 'look', 'out', 'the', 'window', 'at', 'the', 'dull', 'weather']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "sno = SnowballStemmer('english')\n",
    "\n",
    "sno_stemms = [sno.stem(w) for w in re_tokens]\n",
    "print(sno_stemms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hard',\n",
       " 'hardli',\n",
       " 'helpless',\n",
       " 'helplessli',\n",
       " 'hi',\n",
       " 'his',\n",
       " 'slight',\n",
       " 'slightli',\n",
       " 'wa',\n",
       " 'was']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(set(ps_stemms) ^ set(sno_stemms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a4f0234084b86db25ca1539382b5a4944f4886887c3cc83d09ff270c40bf732"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
